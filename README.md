Wanted to actually understand the math behind a neural network instead of just using torch. 
Hence I wrote a very simple model where the activation functions, forward and back propagation, and derivatives were done with just Numpy for Linear Algebra.
Model was trained and tested on the MNIST handwritten digits dataset since thats effectively the "Hello World" of Machine Learning.
